{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNqkKrTDI/RsSJBeRmbAss",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Qk527/Text-generation-with-RNN/blob/main/TextGenerationRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text generation with an RNN"
      ],
      "metadata": {
        "id": "ZD3FQGfqy_NY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m"
      ],
      "metadata": {
        "id": "PADOMEHWzeh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "mkQV3LHcANV1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFPLSZvgwPoX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get data"
      ],
      "metadata": {
        "id": "ph6A3rrNCEZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file(\n",
        "    \"shakespeare.txt\",\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\",\n",
        ")"
      ],
      "metadata": {
        "id": "F8L4qeYdBBg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "062d4e53-f0d6-4f56-9052-fb33deeddc56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "print(f\"Length of text: {len(text)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S8Lfxp_CJxn",
        "outputId": "418cce98-c924-482f-b2ed-1ab3317c6b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First 250 characters\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7WaMDe-CQzZ",
        "outputId": "5b110e95-42f1-4a6b-de9a-9e5e117fcdca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique characters\n",
        "vocab = sorted(set(text))\n",
        "print(f\"{len(vocab)} unique characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNg40C98CVmd",
        "outputId": "2920a3bb-da4c-47ec-837f-ad609ab77e39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process text"
      ],
      "metadata": {
        "id": "8hPhtqXLHJvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorize text"
      ],
      "metadata": {
        "id": "VFQkEJ6rHLrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = [\"abcdefg\", \"xyz\"]\n",
        "\n",
        "# TODO 1\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D09koH3sGTTV",
        "outputId": "c47d0ad5-611b-48f7-e513-e18c5b5e5d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Token to character ID\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "wjTfRCLeHO5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12ORGXLCHQc3",
        "outputId": "4c08aa83-cc23-421a-e4f5-8c884d3b1608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "AsC5ZmFFHWT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pu8RBr7LzHV",
        "outputId": "c0da50e3-75ad-48eb-f9aa-1e0db21f4814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WDLcz3KL0e7",
        "outputId": "7c282b48-7637-48d6-f327-b821984a3fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "9Ve8UnQgL2Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "Zn_FtNWYL3k1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training examples and tagets"
      ],
      "metadata": {
        "id": "aoq0-JNGNfNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 2\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTinf7-cL2_Q",
        "outputId": "73777746-e0de-46a6-cfba-7bad702544b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "MDwTafQqQcFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaITQnlpVVN8",
        "outputId": "9f5e68e9-9fc2-40f0-d6ca-5060f6f659d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)"
      ],
      "metadata": {
        "id": "Ok8UF7z5Vlkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The batch method lets you easily convert these individual characters to sequences of the desired size.\n",
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "    print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e1VtF95WFlF",
        "outputId": "f0de5f67-e463-477a-a2da-4c42fa2e388d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awCGIS9BW5c5",
        "outputId": "ff63b73b-2582-48fa-d5d8-ad39cb474d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input and label are sequences\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "A2ex4JFrW7Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYcM5EyGSObf",
        "outputId": "a9013c55-ad73-4008-be52-b3bda0fcda1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "NPB6DoPgX8rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1pTLD_1csir",
        "outputId": "da1e4555-2f4a-4593-e9c9-8688c940edd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create training batches"
      ],
      "metadata": {
        "id": "JEGVjx8WdPZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset.shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk8S_yWKdOM-",
        "outputId": "2ec38ac3-9bc8-40ad-c356-6ed94bdbd5e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model"
      ],
      "metadata": {
        "id": "c5woybAjdcq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "Iv_jQYKpdTwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        # TODO - Create an embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # TODO - Create a GRU layer\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units, return_sequences=True, return_state=True\n",
        "        )\n",
        "        # TODO - Finally connect it with a dense layer\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = self.embedding(inputs, training=training)\n",
        "        # since we are training a text generation model,\n",
        "        # we use the previous state, in training. If there is no state,\n",
        "        # then we initialize the state\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ],
      "metadata": {
        "id": "6UbUtuWKdgM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        ")"
      ],
      "metadata": {
        "id": "M1Glyzw_dimj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try the model"
      ],
      "metadata": {
        "id": "BuTKVVO0dmBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\n",
        "        example_batch_predictions.shape,\n",
        "        \"# (batch_size, sequence_length, vocab_size)\",\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBxsY3ModkJU",
        "outputId": "29d6f15b-4a8b-4b00-a1f3-b41b24e9f9b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLrCY_7Ndp1y",
        "outputId": "8dc321b3-b7a3-41bc-e101-0c95a2ac3204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(\n",
        "    example_batch_predictions[0], num_samples=1\n",
        ")\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "0ReKr6l4dsZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xmjb-MrO6YZv",
        "outputId": "c79cb1fe-5312-4d5c-88de-273dae54b7e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([63, 38, 17, 42,  2, 23, 34, 26, 60, 28, 48, 43, 40, 63, 10, 64, 49,\n",
              "        9,  1, 48, 55, 13, 58, 29, 26, 39, 65, 15,  8, 49, 13,  2,  1,  0,\n",
              "       36, 20,  5, 60, 62, 48, 46, 20, 52, 40, 43, 54, 34, 24, 43,  7, 50,\n",
              "       19, 21, 57, 51, 53, 45, 35,  5, 45, 35, 19, 50,  3, 28, 21, 19, 53,\n",
              "       55,  3, 61, 13, 20, 54, 14, 59, 18, 15, 35,  1, 36, 60,  2, 28,  2,\n",
              "       41, 31, 36, 45, 55, 15, 36, 26, 46, 16, 45, 42, 30, 13, 49])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgG4cRbU6Z3d",
        "outputId": "fb9f810c-f9c3-4b0a-a50d-80628c5ffc5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"ght,\\nTo cross my obsequies and true love's rite?\\nWhat with a torch! muffle me, night, awhile.\\n\\nROMEO\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b'xYDc JUMuOidax3yj.\\nip?sPMZzB-j? \\n[UNK]WG&uwigGmadoUKd,kFHrlnfV&fVFk!OHFnp!v?GoAtEBV\\nWu O bRWfpBWMgCfcQ?j'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "nyyA1wFj6b5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function here\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "H097XbZx6bX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\n",
        "    \"Prediction shape: \",\n",
        "    example_batch_predictions.shape,\n",
        "    \" # (batch_size, sequence_length, vocab_size)\",\n",
        ")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z32EwDvJ6iNb",
        "outputId": "f179a87f-0750-48a7-b01a-2253c260fca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.190985, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUV1daef6jlI",
        "outputId": "16fc48e3-656d-4a09-ca47-d7bb553465e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.08787"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=loss)"
      ],
      "metadata": {
        "id": "tx-rxus76nrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config checkpoints"
      ],
      "metadata": {
        "id": "P4x749ps6phX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix, save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "Fy4-AYhq6o7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute training"
      ],
      "metadata": {
        "id": "leJ-PmXG6srB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "Ygf57aqz6sLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r76K0qcs6uya",
        "outputId": "4f82c454-e922-4e34-8600-b283c22d9adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 888s 5s/step - loss: 2.7248\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 848s 5s/step - loss: 1.9945\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 905s 5s/step - loss: 1.7159\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 879s 5s/step - loss: 1.5537\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 899s 5s/step - loss: 1.4543\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 1060s 6s/step - loss: 1.3855\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 1057s 6s/step - loss: 1.3334\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 987s 6s/step - loss: 1.2882\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 954s 6s/step - loss: 1.2477\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 1003s 6s/step - loss: 1.2087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate text"
      ],
      "metadata": {
        "id": "yPo3qPPi6wlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            # Put a -inf at each bad index.\n",
        "            values=[-float(\"inf\")] * len(skip_ids),\n",
        "            indices=skip_ids,\n",
        "            # Match the shape to the vocabulary\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())],\n",
        "        )\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        # Convert strings to token IDs.\n",
        "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        # Run the model.\n",
        "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "        predicted_logits, states = self.model(\n",
        "            inputs=input_ids, states=states, return_state=True\n",
        "        )\n",
        "        # Only use the last prediction.\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits / self.temperature\n",
        "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "        # Sample the output logits to generate token IDs.\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        # Convert from token ids to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        # Return the characters and model state.\n",
        "        return predicted_chars, states"
      ],
      "metadata": {
        "id": "KuLqIOSb6vyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "msUfw31L6zgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5yNxujK61Wt",
        "outputId": "189a04ce-fc70-4110-e88e-16fd0a342eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "I never speak, and he's sovereignty well-placed.\n",
            "Go lighten? My metting torments me with time.\n",
            "\n",
            "Third Servingman:\n",
            "Alack, my lord. The music of hull of it!\n",
            "Though nature is the merey daggers by my station,\n",
            "Groclaim mercy, and come; to best you wrong'd;\n",
            "And like a meams that hence with what discourse come\n",
            "The current kind and virtue giddy,\n",
            "Our breed of cloudy deed, and chop it back\n",
            "In treacheth and to me inclined, as is he obey'd.\n",
            "Threatenave is thus; bear the pellictain\n",
            "Of good weeds were pency'd women! let's go night\n",
            "A piten one: thou vantageness, and\n",
            "the nobed hands are no blest and know\n",
            "That Edward king, will over humble treal\n",
            "And aller for a hundred malicians.'\n",
            "\n",
            "Lords:\n",
            "Worthy Clifford? do you love me lie:\n",
            "To be she id the dut of death brothers?' manry,\n",
            "As for a festern persunt than sing;\n",
            "For every banes swell begud;\n",
            "And then conceive in the world's heart the norne,\n",
            "That thou reman'st a one?\n",
            "I am content to earth, and but my legard,\n",
            "As this our inrul at thine eye again;\n",
            "The neety as \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.08221435546875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V34YiOhO7e1v",
        "outputId": "423ef5a1-49b0-463c-f7d7-03e4f5e1a085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nAy, an your welcome of the reason of our mistress,\\nYet death't answering stolence; when thou distess,\\nEven touch that odd, against the waggarness,\\nTo do the new roper warm. Thus, thou drunk was fled\\nFor I expect her bitter chance to this\\nFrick anon, sometimes, gone of those word of\\nFoultenish, make confess,\\nTo badd of correction lie: and thou hast found dishen ears!\\nAh, covert him with a mad?\\n\\nPods:\\nI pray you, list, no long and laid slain\\nWhisperies. God-des your true, in Clease you\\nTo gent to-morrow keep your joy:\\nMy Lord Hurths some times appears, your money son?\\nAlas! good espaye, sid! Rose me of comfort, stands?\\n\\nYORK:\\nThou know'st my father'd, and the horses honesty\\nO, if not accuse those as the world,\\nThat plain in rober-chooligh plucks that torment,\\nOr live and hardly. If our mercy my me for your mire?\\nOne dear bries, which I repeal this country's\\nheep, an assier that hold thus fly.\\nCome, Claudio? speak it not?\\n\\nJOHN OF GAUNT:\\nHe is a hand peace, but in Her motion but\\nHeaven a\"\n",
            " b\"ROMEO:\\nWhat, hath alone kind your lordship?\\n\\nAEdile:\\nI must not want the breath of Carilau his birth,\\nWas he we have done.\\n\\nLEONTES:\\nSay he co?\\n\\nBUCKINGHAM:\\nWill't scare, my lord. Come, sir! do you tell me we;\\nand weaven as the penace of before heir to\\nBeing a flower crowns for thine antirs\\nAnd bark phell's voices Capable me,\\nAnd I to-follow 'sute, that of dust!\\nThink for news, more senators For thy unnatural,\\nTo bear her fair Baptians, was thy kindness' heart,\\nWe shall a dream of cursing of,\\nThat which thou didst challen thy breast of county\\nAfter a sword that Warwick must extreme others.\\nThou art welcome. Welp, madam: good night, Lady Bon, How calubals.\\n\\nROMEO:\\nSpoke false, and that the famous flesh\\nThy little blood upon my brother's;\\nEven like the pestilence, point gentlemen 's alliance to her.\\n\\nThird Citizen:\\nMethinks have your disconcent be spent at high off.\\nSidul; I see him will BENCAULET:\\nI have needly cold fill.\\n\\nFirst Lord:\\nHave done?\\nWent not we saves in the town belike it o't\\nAnd\"\n",
            " b\"ROMEO:\\nYield grays for purpises, chance could you a\\ngross: tell me you make your part of years\\nFor sweetings, for a most unnaples a\\ndream of them a piggu, of thy commends,\\nBut on my steem'd no protule nine of worth me advore\\nMore had but milder: good haste. No, sir,\\nI did full speak, sir.\\n\\nPOMPEY:\\nSo morrow and Towelve; and too myself,\\nWhich now so favour as that elves it here,\\nTo fost the black flowers circum to the to\\nchange to supplact of womb'st it!\\nAnd if every weep kind. Those goes whose robes!\\n\\nChild be holp'd up from foseign or high England.\\nThey cannot be put to the couplex of sut,\\nFor the lib, cried 'gains your strugght nich mourning lord.\\n\\nJULIET:\\nThereat is no soldier; nay, cousin welcome, do.\\nI pry it; But I'll be boy:\\nWelce, us, sir?\\nHe eat; Some hour effearted me!\\nHow gallented Richard look my foe fetch thy counsel\\nThat breed in any mother Richmonds,\\nTalks will before your grace much more shall speak.\\nTo couran of Neptuan, speak not well?\\nGod was a but of bleeding of them only\"\n",
            " b\"ROMEO:\\nThou rave! it nog any thy wrongs of long so\\nOn of peace, believe me, sir, I am your waghard\\nmy brother breathes: hep sit up for enemy,\\nThat you bet establise ye, shall we come to hear me\\nFroth, fuils too: as if thou queen,\\nOr who is my wrong, thrice crown'd with falsehood\\nWhich show more pitied and\\nhis left and earsh, and virtue it not\\nYou want no offrace: to save those thrifty\\nLike noblong-hearted crowns of Richard will be late to\\nsceal or weep; so; my good night! what was answering\\nTybalt, so how could now go cleed from death.\\n\\nISABELLA:\\nAy, but the people grief shall we see that touch'd,\\nIf any but banish'd from risudes? what is foo\\nnot condition, avenended. This is the nighting town, gentle\\nMust a pod to aft, deserved in the world.\\n\\nSurdent:\\nSo, now there len thee saints, he is coming;\\nWhat Edward did, I say, for the reace:\\nI stand be thus I look sweet father have\\nmade perfection pluck it confound about,\\nAs friends as I would pluck'd you from your\\nwouse to a feast-about simple bra\"\n",
            " b\"ROMEO:\\n\\nEARL OF BULLI: My pail of dark!\\nNow piey his graves: and, let us head;'; to make honesterier may.\\nIn brief that makes these burds blood\\nTransor, not for pluck mine own: if\\nthen thou wert stad ta'en, wilt trubble true I'll be\\nwill die through the heart thine eyes from sive rans;\\nThy aitill draw with disgounce, of men clum.\\n\\nJULIET:\\nI make with royals; for 'tis pleased, even from else the\\nMows roof and I'll try inveraugatly by the sey\\nWord unto the sun which he's sigheries again.\\n\\nTRANIO:\\nSir Richard Mortague, but not a sweet?\\n\\nQUEEN ELIZABETH:\\nFor is a fled-heart. Come, fay, Somey, I say, though I the marriage\\nAs he in such voluma certain.\\n\\nWARWICK:\\nTrumpent, and Six Coriola Montague Burgun, 'sit in his holy means\\nTo be ruled better through these privalems and his\\nknittle proclaim and supplain and rebellion\\nBy pagention of her incle yield this deedly heavy,\\nThat our policy holds' conquest,\\nAnd spury, by view, for cruel heart to hire;\\nO my desires, that look for looking speed\\nA kind in\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.373613595962524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export generator"
      ],
      "metadata": {
        "id": "XNGJt6iJ7hiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, \"one_step\")\n",
        "one_step_reloaded = tf.saved_model.load(\"one_step\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1U8hKBS7gtB",
        "outputId": "863bb9a0-3e54-4b74-e66a-5e6d33548e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fc256cedd20>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:absl:Found untraced functions such as _update_step_xla, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "    next_char, states = one_step_reloaded.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBPKwKdV7kV-",
        "outputId": "6e450c5c-3f4e-4b2e-9c8a-c15ee910b604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "We will with thee. Ah, put since you?\n",
            "why, boy! why, then go say: for wish very time to speak we\n",
            "pr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced: Customized Training"
      ],
      "metadata": {
        "id": "XR--w1JQCRiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "    @tf.function\n",
        "    def train_step(self, inputs):\n",
        "        inputs, labels = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = self.loss(labels, predictions)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        return {\"loss\": loss}"
      ],
      "metadata": {
        "id": "X5fUK8nO7lZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        ")"
      ],
      "metadata": {
        "id": "2e34pFOkMPbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        ")"
      ],
      "metadata": {
        "id": "fq757HTgM9qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-OQ4t1KUThZ",
        "outputId": "c0de5337-9f19-4fe9-eb25-bd354bb2fe91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 1016s 6s/step - loss: 2.7019\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc24ebe1960>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for batch_n, (inp, target) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs[\"loss\"])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = (\n",
        "                f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            )\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f\"Epoch {epoch+1} Loss: {mean.result().numpy():.4f}\")\n",
        "    print(f\"Time taken for 1 epoch {time.time() - start:.2f} sec\")\n",
        "    print(\"_\" * 80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Da2Fk2tUsFC",
        "outputId": "4054e718-cfcc-4016-8b8c-d17bcc7705f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1822\n",
            "Epoch 1 Batch 50 Loss 2.0379\n",
            "Epoch 1 Batch 100 Loss 1.9265\n",
            "Epoch 1 Batch 150 Loss 1.8018\n",
            "\n",
            "Epoch 1 Loss: 1.9735\n",
            "Time taken for 1 epoch 981.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.7936\n",
            "Epoch 2 Batch 50 Loss 1.7157\n",
            "Epoch 2 Batch 100 Loss 1.6551\n",
            "Epoch 2 Batch 150 Loss 1.6149\n",
            "\n",
            "Epoch 2 Loss: 1.6951\n",
            "Time taken for 1 epoch 921.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5663\n",
            "Epoch 3 Batch 50 Loss 1.5554\n",
            "Epoch 3 Batch 100 Loss 1.5218\n",
            "Epoch 3 Batch 150 Loss 1.5102\n",
            "\n",
            "Epoch 3 Loss: 1.5377\n",
            "Time taken for 1 epoch 921.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4638\n",
            "Epoch 4 Batch 50 Loss 1.4724\n",
            "Epoch 4 Batch 100 Loss 1.4242\n",
            "Epoch 4 Batch 150 Loss 1.4402\n",
            "\n",
            "Epoch 4 Loss: 1.4430\n",
            "Time taken for 1 epoch 981.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3920\n",
            "Epoch 5 Batch 50 Loss 1.3703\n",
            "Epoch 5 Batch 100 Loss 1.3712\n",
            "Epoch 5 Batch 150 Loss 1.3579\n",
            "\n",
            "Epoch 5 Loss: 1.3764\n",
            "Time taken for 1 epoch 982.05 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3029\n",
            "Epoch 6 Batch 50 Loss 1.3813\n",
            "Epoch 6 Batch 100 Loss 1.3030\n",
            "Epoch 6 Batch 150 Loss 1.2691\n",
            "\n",
            "Epoch 6 Loss: 1.3256\n",
            "Time taken for 1 epoch 1041.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2796\n",
            "Epoch 7 Batch 50 Loss 1.2967\n",
            "Epoch 7 Batch 100 Loss 1.2686\n",
            "Epoch 7 Batch 150 Loss 1.3247\n",
            "\n",
            "Epoch 7 Loss: 1.2815\n",
            "Time taken for 1 epoch 921.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2185\n",
            "Epoch 8 Batch 50 Loss 1.2564\n",
            "Epoch 8 Batch 100 Loss 1.2302\n",
            "Epoch 8 Batch 150 Loss 1.2606\n",
            "\n",
            "Epoch 8 Loss: 1.2410\n",
            "Time taken for 1 epoch 921.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1627\n",
            "Epoch 9 Batch 50 Loss 1.2136\n",
            "Epoch 9 Batch 100 Loss 1.1789\n",
            "Epoch 9 Batch 150 Loss 1.2549\n",
            "\n",
            "Epoch 9 Loss: 1.2019\n",
            "Time taken for 1 epoch 921.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1370\n",
            "Epoch 10 Batch 50 Loss 1.1307\n",
            "Epoch 10 Batch 100 Loss 1.1703\n",
            "Epoch 10 Batch 150 Loss 1.1490\n",
            "\n",
            "Epoch 10 Loss: 1.1621\n",
            "Time taken for 1 epoch 922.05 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSBcQ_TVoekI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}